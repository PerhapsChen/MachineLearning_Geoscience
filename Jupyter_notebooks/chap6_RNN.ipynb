{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caefbebb",
   "metadata": {},
   "source": [
    "# Chapter 6: Reccurrent Neural Networks (RNN)\n",
    "\n",
    "Recurrent Neural Networks (RNN) can read inputs $x^{\\langle t \\rangle}$ one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs (A bidirection RNN, outside the scope of the present notebook, can take context from both the past and the future). \n",
    "\n",
    "Simple, so-called 'Vanilla', RNNs are rarely used in practice due to the _vanishing gradient problem_. However, it is worthwhile to develop the simplest possible RNN model to understand the underlying concepts that also apply to LSTM and GRU architectures. To illustrate RNN predictions, we must use very short time series. We will use a character-level language model to generate mineral names from a dataset of existing mineral names (example of many-to-many RNN classification).\n",
    "\n",
    "You will first build a Vanilla RNN from scratch, and second use an LSTM defined with Pytorch so that you can get a feeling of the data and model structure needed in more sophisticated applications. You will predict mineral names in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1cfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e81bb",
   "metadata": {},
   "source": [
    "## 1. Mineral name dataset & character dictionary\n",
    "\n",
    "We will first open the dataset of all known mineral names and then create the python dictionary `char_to_ix` (i.e., a _hash table_) to map each character to an index. We also create a second python dictionary `ix_to_char` that maps each index back to the corresponding character to later translate the probability distribution output of the softmax layer into the generated text.\n",
    "\n",
    "<img src=\"figs_notebook/minerals.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> Source: National Museums Scotland.</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd7769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 14631 total characters and 27 unique characters.\n",
      "Unique characters are:  ['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Dictionary (index to character): {0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "\n",
      "Character dataset excerpt (first 300 characters):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'abelsonite\\nabernathyite\\nabhurite\\nabramovite\\nabswurmbachite\\nacanthite\\nachavalite\\nactinolite\\nacuminite\\nadamite\\nadelite\\nadmontite\\naegirine\\naenigmatite\\naerinite\\naerugite\\nafghanite\\nafwillite\\nagardite\\nagrellite\\nagrinierite\\naguilarite\\naheylite\\nahlfeldite\\naikinite\\najoite\\nakaganeite\\nakatoreite\\nakdalaite\\naker'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('data/minerals.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('The dataset contains %d total characters and %d unique characters.' % (data_size, vocab_size))\n",
    "print('Unique characters are: ', sorted(chars))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print('Dictionary (index to character):', ix_to_char)\n",
    "print('\\nCharacter dataset excerpt (first 300 characters):')\n",
    "data[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06496b09",
   "metadata": {},
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\" (or newline) character, which represents the end of a mineral name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8a5c8",
   "metadata": {},
   "source": [
    "## 2. RNN from scratch\n",
    "\n",
    "Our RNN model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients (specific to RNNs)\n",
    "    - Update parameter with gradient descent update rule\n",
    "    - Sample characters to initiate new name generation\n",
    "- Return learned parameters\n",
    "\n",
    "The overall structure is similar to the one learned in chapter 4 for feedforward ANNs. The main difference is that the hidden layer is here a recursion on one cell, in time: At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$ (Fig. 1).\n",
    "\n",
    "<img src=\"figs_notebook/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> <b>Fig. 1</b>: Many-to-many RNN.</center></caption>\n",
    "\n",
    "Another RNN specificity is _gradient clipping_ to avoid exploding gradients. Note also that to see the output of such a RNN model, we need to develop a sampling procedure that will generate new text $y$ based on previous text $x$. This will be detailed in a specific section below.\n",
    "\n",
    "Figure 2 illustrates the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"figs_notebook/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> <b>Fig. 2</b>: Vanilla RNN cell.</center></caption>\n",
    "    \n",
    "The cell takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ via the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551a135",
   "metadata": {},
   "source": [
    "### 2.1. Parameter initialization\n",
    "\n",
    "**EXERCISE 1:** initialize the model parameters with small random values using `np.random.randn(n1, n2)*0.01` for weights and `np.zeros((n, 1))` for biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0163abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        b --  Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "#    Wax = ?\n",
    "#    Waa = ?\n",
    "#    Wya = ?\n",
    "#    b = ?\n",
    "#    by = ?\n",
    "    \n",
    "    \n",
    "    # SOLUTION\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1))               # hidden bias\n",
    "    by = np.zeros((n_y, 1))              # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c6cbf",
   "metadata": {},
   "source": [
    "### 2.2. Forward propagation\n",
    "\n",
    "We will first define the RNN cell, and then the loop in time. Let us already define the `softmax` activation of the RNN (we will use `np.tanh` for the hyperbolic tangent function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f529d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c565563",
   "metadata": {},
   "source": [
    "We are now going to implement the computation for a single time-step (i.e. RNN cell, see Fig. 2).\n",
    "\n",
    "**EXERCISE 2**: Implement the RNN cell described in Figure 2.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$.\n",
    "3. Return $a^{\\langle t \\rangle}$ and $y^{\\langle t \\rangle}$\n",
    "\n",
    "We will vectorize over $m$ examples. Thus, $x^{\\langle t \\rangle}$ will have dimension $(n_x,m)$, and $a^{\\langle t \\rangle}$ will have dimension $(n_a,m)$. _Hint:_ use `np.dot` for the dot product of two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57ffa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename rnn_cell_forward\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure 2\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    b = parameters[\"b\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # YOUR CODE HERE - compute next activation state using the formula given above\n",
    "#    a_next = ?\n",
    "    \n",
    "    # SOLUTION\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)          # hidden state\n",
    "\n",
    "    # YOUR CODE HERE - compute output of the current cell\n",
    "#    p_t = ?\n",
    "    \n",
    "    # SOLUTION\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)                             # probabilities for next characters\n",
    "    \n",
    "    # YOUR CODE HERE - SOLUTION\n",
    "#    return ?, ?\n",
    "\n",
    "    # SOLUTION\n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059803da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "a_next, yt_pred = rnn_step_forward(parameters, a_prev, xt)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9542598",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a_next[4]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt[1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b87f4",
   "metadata": {},
   "source": [
    "We will now run `rnn_step_forward` $T_x$ times while updating the loss at each time increment $t$.\n",
    "\n",
    "<img src=\"figs_notebook/rnn2.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> <b>Fig. 3</b>: Many-to-many RNN with cell details.</center></caption>\n",
    "\n",
    "**EXERCISE 3:** Complete the forward propagation of the RNN described in Figure 3, by updating the \"next\" hidden state and the cache by running `rnn_cell_forward`. _Hint:_ be careful with the time increments of the different inputs of the RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831584d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    x, a, y_hat = {}, {}, {}       # Initialize x, a and y_hat as empty dictionaries\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0                       # initialize your loss to 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # YOUR CODE HERE - Run one step forward of the RNN\n",
    "#        a[t], y_hat[t] = ?\n",
    "        \n",
    "        # SOLUTION\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf2c18",
   "metadata": {},
   "source": [
    "### 2.3. Backward propagation\n",
    "\n",
    "When we implemented a simple (fully connected) neural network in chapter 4, we used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in RNNs we can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated. They are only given below for completeness.\n",
    "\n",
    "We will start by computing the backward pass for the Vanilla RNN-cell.\n",
    "\n",
    "<img src=\"figs_notebook/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> <b>Fig. 4</b>: RNN-cell's backward pass.</center></caption>\n",
    "    \n",
    "Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$.\n",
    "\n",
    "To compute the `rnn_step_backward`, we need to compute the following equations. The derivative of $\\tanh$ is $1-\\tanh(x)^2$. Note that: $ \\text{sech}(x)^2 = 1 - \\tanh(x)^2$\n",
    "\n",
    "Similarly for $\\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{ax}}, \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{aa}},  \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial b}$, the derivative of  $\\tanh(u)$ is $(1-\\tanh(u)^2)du$. \n",
    "\n",
    "The final two equations also follow the same rule and are derived using the $\\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0278e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \"\"\"\n",
    "    TO MODIFY - NOT MATCHING\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradients of input data, of shape (n_x, m)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0326bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\"\n",
    "    MODIFY TEXT...\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
    "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
    "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
    "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56fba9",
   "metadata": {},
   "source": [
    "### 2.4. Gradient clipping\n",
    "\n",
    "We will now implement the `clip` function that will be called between the backward pass and the parameter update. This will make sure that the RNN gradients are not \"exploding\" (i.e. overly large values). \n",
    "\n",
    "There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range $[-N, N]$. More generally, you will provide a `maxValue` (say 10).\n",
    "\n",
    "<img src=\"figs_notebook/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> <b>Fig. 5</b>: Visualization of gradient descent with & without gradient clipping, in a case where the network is running into slight \"exploding gradient\" problems. </center></caption>\n",
    "\n",
    "**EXERCISE 4:** Implement the function below to return the clipped gradients of your dictionary `gradients`. Your function takes in a maximum threshold and returns the clipped versions of your gradients. Use `np.clip(a, a_min, a_max, out)`. `out` is the array where the results will be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68fca9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- clipping value\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # YOUR CODE HERE: clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "\n",
    "    \n",
    "    \n",
    "    # SOLUTION\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "\n",
    "        \n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27e668",
   "metadata": {},
   "source": [
    "### 2.5. Parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f5d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    parameters['Wax'] += -learning_rate * gradients['dWax']\n",
    "    parameters['Waa'] += -learning_rate * gradients['dWaa']\n",
    "    parameters['Wya'] += -learning_rate * gradients['dWya']\n",
    "    parameters['b']  += -learning_rate * gradients['db']\n",
    "    parameters['by']  += -learning_rate * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403556b",
   "metadata": {},
   "source": [
    "### 2.6. Sampling\n",
    "\n",
    "Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:\n",
    "\n",
    "<img src=\"figs_notebook/sampling.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> <b>Fig. 6</b>: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>\n",
    "\n",
    "Four steps are carried (all is already done for you). Still, read carefully about the process:\n",
    "\n",
    "- **Step 1**: Pass the network the first \"dummy\" input $x^{\\langle 1 \\rangle} = \\vec{0}$ (the vector of zeros). This is the default input before we've generated any characters. We also set $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "\n",
    "- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  We have provided a `softmax()` function that you can use.\n",
    "\n",
    "- **Step 3**: Carry out sampling: Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. To implement it, you can use [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html).\n",
    "\n",
    "Here is an example of how to use `np.random.choice()`:\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "```\n",
    "This means that you will pick the `index` according to the distribution: \n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "\n",
    "- **Step 4**: The last step to implement in `sample()` is to overwrite the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. You will represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character you've chosen as your prediction. You will then forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until you get a \"\\n\" character, indicating you've reached the end of the mineral name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e203e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter+seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b95f0",
   "metadata": {},
   "source": [
    "### 2.7. RNN model wrap-up & word generator\n",
    "\n",
    "**EXERCISE 5:** Fill the `rnn_model` function that calls the functions doing the following: \n",
    "\n",
    "- Forward propagation to compute the loss function\n",
    "- Backward propagation to compute the gradients with respect to the loss function\n",
    "- Clip the gradients to avoid exploding gradients (specific to RNNs)\n",
    "- Update parameter with gradient descent update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf912dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(X, Y, a_prev, parameters, vocab_size, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Forward propagate through time (≈1 line)\n",
    "#    loss, cache = ?\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "#    gradients, a = ?\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "#    gradients = ?\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "#    parameters = ?\n",
    "    \n",
    "    \n",
    "    # SOLUTION\n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters, vocab_size)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, maxValue=5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c84fb6",
   "metadata": {},
   "source": [
    "Given the dataset of mineral names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. The first entry of `X` being `None` will be interpreted by `rnn_forward()` as setting $x^{\\langle 0 \\rangle} = \\vec{0}$. Further, this ensures that `Y` is equal to `X` but shifted one step to the left, and with an additional \"\\n\" appended to signify the end of the mineral name. \n",
    "\n",
    "**EXERCISE 6:** Fill the gaps in `word_generator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58cf66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_generator(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, mineral_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates mineral names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    mineral_names -- number of mineral names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE - Retrieve n_x and n_y from vocab_size\n",
    "#    n_x, n_y = ?, ?\n",
    "    \n",
    "    \n",
    "    # SOLUTION\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "\n",
    "    \n",
    "    # YOUR CODE HERE - Initialize parameters\n",
    "#    parameters = initialize_parameters(?, ?, ?)\n",
    "    \n",
    "    \n",
    "    # SOLUTION\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    seq_length = mineral_names\n",
    "    loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "    \n",
    "    # Build list of all mineral names (training examples).\n",
    "    with open(\"data/minerals.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "\n",
    "    np.random.seed(0)\n",
    "    # YOUR CODE HERE - Shuffle list of all mineral names\n",
    "#    np.random.shuffle(?)\n",
    "\n",
    "\n",
    "    # SOLUTION\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    \n",
    "    # Initialize the hidden state of your RNN\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "#        curr_loss, gradients, a_prev = rnn_model(...)\n",
    "        \n",
    "        \n",
    "        # SOLUTION\n",
    "        curr_loss, gradients, a_prev = rnn_model(X, Y, a_prev, parameters, vocab_size, learning_rate = 0.01)\n",
    "        \n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = loss * 0.999 + curr_loss * 0.001\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of mineral names to print\n",
    "            seed = 0\n",
    "            for name in range(mineral_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                #print_sample(sampled_indices, ix_to_char)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in sampled_indices)\n",
    "                txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "                print ('%s' % (txt, ), end='')\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6ed6d",
   "metadata": {},
   "source": [
    "Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, the RNN model should learn to generate reasonable-looking mineral names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a91ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087337\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjjjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjjjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjjjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjjjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 23.513108\n",
      "\n",
      "Liutordinite\n",
      "Ite\n",
      "Iusrodirite\n",
      "Lacagoshabite\n",
      "Xtordionitikite\n",
      "Caadoshbanrite\n",
      "Utoenhitelgorrarite\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 21.846217\n",
      "\n",
      "Liusorgionite\n",
      "Hled\n",
      "Hwssohite\n",
      "Lcabenphackomoite\n",
      "Xtrofite\n",
      "Caaglokbansite\n",
      "Troelite\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 21.267887\n",
      "\n",
      "Lhwtsselhopeuite\n",
      "Ite\n",
      "Iustogite\n",
      "Lacaite\n",
      "Xutrenglirthorr\n",
      "Caadotfackrite\n",
      "Utrengkite\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 20.922784\n",
      "\n",
      "Lewtroginite\n",
      "Hlacaite\n",
      "Hytssengorevite\n",
      "Lacagrolachsite\n",
      "Wssrdinite\n",
      "Baadrolachsite\n",
      "Ussclerinternsarite\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 20.697659\n",
      "\n",
      "Lewtrodite\n",
      "Hifdaite\n",
      "Hustlenenite\n",
      "Lcaberrite\n",
      "Wrroelite\n",
      "Cabersincite\n",
      "Ussengite\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 20.526648\n",
      "\n",
      "Leytrocite\n",
      "Erdaaepokacite\n",
      "Guttoeite\n",
      "Lacagroiicite\n",
      "Wrrocite\n",
      "Acagroiicite\n",
      "Ussbiglinsite\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 20.485592\n",
      "\n",
      "Inutorite\n",
      "Epecaite\n",
      "Evstilite\n",
      "Ied\n",
      "Vossenite\n",
      "Acagriod\n",
      "Tsscinite\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 20.381393\n",
      "\n",
      "Lewtophinkite\n",
      "Gladaite\n",
      "Hvstoite\n",
      "Lacagorite\n",
      "Vuttenite\n",
      "Acaforipcite\n",
      "Uttanellite\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 20.304972\n",
      "\n",
      "Leytroelite\n",
      "Erdaberoite\n",
      "Futtoite\n",
      "Lacaite\n",
      "Vtssenite\n",
      "Acaite\n",
      "Ussenite\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 20.268828\n",
      "\n",
      "Lewsskelite\n",
      "Eree\n",
      "Gwrroeite\n",
      "Ladalkite\n",
      "Vusselite\n",
      "Babersilallite\n",
      "Usselite\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 20.198846\n",
      "\n",
      "Leytorite\n",
      "Ffeiaite\n",
      "Hystoite\n",
      "Ladallite\n",
      "Votteogniite\n",
      "Adallite\n",
      "Tsolite\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 20.065250\n",
      "\n",
      "Lawtoreite\n",
      "Ffehaite\n",
      "Hystine\n",
      "Ladaite\n",
      "Wustenite\n",
      "Acaite\n",
      "Usselfite\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 20.187033\n",
      "\n",
      "Meutorite\n",
      "Hofcaite\n",
      "Hwusthinite\n",
      "Macafote\n",
      "Vussengite\n",
      "Cabetrite\n",
      "Usselite\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 20.016177\n",
      "\n",
      "Meytsokite\n",
      "Hne\n",
      "Hystoeite\n",
      "Mafaite\n",
      "Vussglenite\n",
      "Cabbroite\n",
      "Ttrenite\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 20.106320\n",
      "\n",
      "Meutorite\n",
      "Hoedaite\n",
      "Hustodite\n",
      "Macafotile\n",
      "Vussenite\n",
      "Caberogdennite\n",
      "Ussenite\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 20.006800\n",
      "\n",
      "Leytrocite\n",
      "Hiegaite\n",
      "Hwsspdipkite\n",
      "Lacalloipamonndite\n",
      "Vroseldite\n",
      "Babcorite\n",
      "Ttokite\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 19.990772\n",
      "\n",
      "Lawtrodite\n",
      "Gelcaite\n",
      "Hussolite\n",
      "Ladaite\n",
      "Votterite\n",
      "Baagrolacite\n",
      "Sssenite\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = word_generator(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002cd59",
   "metadata": {},
   "source": [
    "**EXERCISE 7:** Interpret the RNN results by making observations about the evolution of results over training and how realistic the generated names look at the end (by comparing with patterns observed in the real mineral name database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5d7de",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "SOLUTION\n",
    "\n",
    "* We see that the words generated by the RNN are at first random. Very rapidly, realistic names emerge.\n",
    "* Most existing mineral names end with 'ite' (have a look at the database we used as input). With such high probability, the RNN ends most of the generated words with 'ite'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e757e9",
   "metadata": {},
   "source": [
    "## 3. LSTM on Pytorch\n",
    "\n",
    "We'll start off by importing the main PyTorch package and some important sub-libraries. We will then format the input data to solve the same problem as in section 2, but now with an LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5149c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.functional import F\n",
    "\n",
    "import math\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1169406",
   "metadata": {},
   "source": [
    "### 3.1. Input reformatting\n",
    "\n",
    "Here, we need to reformat the data by ending each word with the \"< EOS >\" marker (meaning end of sequence/sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02a56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_names(fname):\n",
    "    EOS = \"<EOS>\"\n",
    "    data = []\n",
    "        \n",
    "    with open(fname) as file:\n",
    "        text = file.read().lower()\n",
    "            \n",
    "    names = text.splitlines()\n",
    "    for i, name in enumerate(names):\n",
    "        # Split names to chars and append the End of Sequence (EOS) Token\n",
    "        ch_list = list(name) + [EOS]\n",
    "        data.append(ch_list)\n",
    "    return data\n",
    "\n",
    "data_in_char = split_to_names(\"data/minerals.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edae4fd",
   "metadata": {},
   "source": [
    "**EXERCISE 8:** Redefine the vocabulary `char_to_ix` and `ix_to_char` with the added special symbol. _Hint:_ We already did it in section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec282b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "char_vocab = [\"<EOS>\"] + sorted([ch for ch in string.ascii_lowercase])\n",
    "#char_to_ix = ?\n",
    "#ix_to_char = ?\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(char_vocab)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(char_vocab)) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87305f6a",
   "metadata": {},
   "source": [
    "Pytorch requires a special data format for loading inputs into its models. It is in two parts and involves first converting our data from strings (characters) to integers as our model understands only numbers. And the second part is the logic of how we will be loading our data to train our model. We will be using Pytorch’s `Dataset` and `DataLoader` class to handle both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c058d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_as_str, _map):\n",
    "        self.data_as_int = []\n",
    "        \n",
    "        # Convert characters to integers\n",
    "        for seq_as_str in data_as_str:\n",
    "            seq_as_int = keys_to_values(seq_as_str, _map,\n",
    "                random.choice(list(_map)))\n",
    "            \n",
    "            self.data_as_int.append(seq_as_int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_as_int)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        # Get data sample at index ix\n",
    "        item = self.data_as_int[ix]\n",
    "        \n",
    "        # Slice x and y from sample\n",
    "        x = item[:-1]\n",
    "        y = item[ 1:]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "def keys_to_values(keys, _map, default):\n",
    "    return [_map.get(key, default) for key in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7656284",
   "metadata": {},
   "source": [
    "Above, we first defined a Python class that inherits the Dataset class. This class will be responsible for fetching samples from our data. Next, we defined the `__init__` method, which takes in our already preprocessed data and converts each character to an integer using one of the dictionaries we created previously. Now our data is of a type our model can understand.\n",
    "Pytorch’s Dataset class then requires that we define two other methods. These are the `__len__` method (which returns the length of our data), and the `__getitem__` method (which returns a sample at a particular index in our data). Implementing the `__len__` method is as simple as calling Python’s built-in function, `len`, on our data and returning the value. So that leaves us with the `__getitem__` method.\n",
    "The `__getitem__` method receives the index we are interested in automatically as an argument. We then get the sample at that index in our data, slice out X (which is from the first value to the second value from the back) and Y (which is from the second value to the last value) from the sample, convert them to tensors and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f26faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data_in_char, char_to_ix)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2aac1",
   "metadata": {},
   "source": [
    "Lastly, we created a DataLoader object. This object provides a wrapper over our dataset that allows us to do (much easily) things like shuffling our dataset and more complex things like combining multiple datasets or using multiple workers to load our dataset. Luckily, we will not be dealing with any of the complex cases here. We finished by setting batch_size to 1 and shuffle to True.\n",
    "\n",
    "### 3.2. LSTM model definition & training\n",
    "\n",
    "LSTM model definition in Pytorch is shown below. It gives you an idea of the very different architecture and python code needed compared to Tensorflow 2 / Keras for example. Simply run the next cell to define your LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b00bc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, _map, hidden_size, emb_dim=8, n_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size  = len(_map)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim     = emb_dim\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim =self.emb_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size =self.emb_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers =self.n_layers,\n",
    "            batch_first=True)\n",
    "                \n",
    "        self.fc = nn.Linear(\n",
    "            in_features =self.hidden_size,\n",
    "            out_features=self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        n_b, n_s = x.shape\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        yhat, state = self.lstm(embed, prev_state)\n",
    "        \n",
    "        out = self.fc(yhat)\n",
    "        return out, state\n",
    "    \n",
    "    def init_state(self, b_size=1):\n",
    "        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, b_size, self.hidden_size))\n",
    "\n",
    "model = Model(char_to_ix, 64, 8, n_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee3a4c",
   "metadata": {},
   "source": [
    "Now run the next cell to train the Pytorch LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d9a74e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   1000/35000, Loss:   1.9059\n",
      "Iteration:   2000/35000, Loss:   1.7706\n",
      "Iteration:   3000/35000, Loss:   1.7170\n",
      "Iteration:   4000/35000, Loss:   1.6809\n",
      "Iteration:   5000/35000, Loss:   1.6480\n",
      "Iteration:   6000/35000, Loss:   1.6098\n",
      "Iteration:   7000/35000, Loss:   1.5843\n",
      "Iteration:   8000/35000, Loss:   1.5461\n",
      "Iteration:   9000/35000, Loss:   1.5271\n",
      "Iteration:  10000/35000, Loss:   1.4923\n",
      "Iteration:  11000/35000, Loss:   1.4978\n",
      "Iteration:  12000/35000, Loss:   1.4601\n",
      "Iteration:  13000/35000, Loss:   1.4288\n",
      "Iteration:  14000/35000, Loss:   1.4149\n",
      "Iteration:  15000/35000, Loss:   1.4187\n",
      "Iteration:  16000/35000, Loss:   1.3768\n",
      "Iteration:  17000/35000, Loss:   1.3804\n",
      "Iteration:  18000/35000, Loss:   1.3681\n",
      "Iteration:  19000/35000, Loss:   1.3775\n",
      "Iteration:  20000/35000, Loss:   1.3261\n",
      "Iteration:  21000/35000, Loss:   1.3359\n",
      "Iteration:  22000/35000, Loss:   1.3350\n",
      "Iteration:  23000/35000, Loss:   1.3237\n",
      "Iteration:  24000/35000, Loss:   1.2861\n",
      "Iteration:  25000/35000, Loss:   1.2993\n",
      "Iteration:  26000/35000, Loss:   1.3015\n",
      "Iteration:  27000/35000, Loss:   1.2880\n",
      "Iteration:  28000/35000, Loss:   1.2991\n",
      "Iteration:  29000/35000, Loss:   1.2631\n",
      "Iteration:  30000/35000, Loss:   1.3333\n",
      "Iteration:  31000/35000, Loss:   1.2803\n",
      "Iteration:  32000/35000, Loss:   1.2384\n",
      "Iteration:  33000/35000, Loss:   1.2908\n",
      "Iteration:  34000/35000, Loss:   1.2839\n",
      "Iteration:  35000/35000, Loss:   1.2800\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_iter, criterion, clip=0.25, lr=0.001, print_every=50):\n",
    "    model.train()\n",
    "    \n",
    "    costs = []\n",
    "    running_loss = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    curr_iter = 0\n",
    "    while curr_iter<num_iter:\n",
    "        for x, y in data:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Initialise model's state and perform forward-prop\n",
    "            prev_state = model.init_state(b_size=x.shape[0])\n",
    "            out, state = model(x, prev_state)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(out.transpose(1, 2), y)\n",
    "            costs.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate gradients and update parameters\n",
    "            loss.backward()\n",
    "            if clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_iter += 1\n",
    "            if print_every and (curr_iter%print_every)==0:\n",
    "                print(\"Iteration: {:{}}/{}, Loss: {:8.4f}\".format(\n",
    "                    curr_iter, int(math.log(num_iter, 10))+2, num_iter,\n",
    "                    running_loss/float(print_every)))\n",
    "                running_loss = 0\n",
    "                \n",
    "            if curr_iter>=num_iter:\n",
    "                break\n",
    "    return model, costs\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model, costs = train(\n",
    "    model, dataloader, 35000, criterion, clip=5, lr=0.01, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc8818",
   "metadata": {},
   "source": [
    "### 3.3. LSTM word generator\n",
    "\n",
    "Similarly to the vanilla RNN, we need to apply a sampling method to generate new sequences of letters that will for our mineral names. This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9237b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(model, x, prev_state, topk=5, uniform=True):\n",
    "    # Perform forward-prop and get the output of the last time-step\n",
    "    out, state = model(x, prev_state)\n",
    "    last_out = out[0, -1, :]\n",
    "\n",
    "    # Get the top-k indexes and their values\n",
    "    topk = topk if topk else last_out.shape[0]\n",
    "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
    "    \n",
    "    # Get the softmax of the topk's and sample\n",
    "    p = None if uniform else F.softmax(top_logit.detach(), dim=-1).numpy()\n",
    "    sampled_ix = np.random.choice(top_ix, p=p)\n",
    "    return sampled_ix, state\n",
    "\n",
    "\n",
    "def sample(model, seed, topk=5, uniform=True, max_seqlen=18, stop_on=None):\n",
    "    seed = seed if isinstance(seed, (list, tuple)) else [seed]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sampled_ix_list = seed[:]\n",
    "        x = torch.tensor([seed])\n",
    "        \n",
    "        prev_state = model.init_state(b_size=1)\n",
    "        for t in range(max_seqlen - len(seed)):\n",
    "            sampled_ix, prev_state = sample_next(model, x, prev_state, topk, uniform)\n",
    "\n",
    "            sampled_ix_list.append(sampled_ix)\n",
    "            x = torch.tensor([[sampled_ix]])\n",
    "            \n",
    "            if sampled_ix==stop_on:\n",
    "                break\n",
    "    \n",
    "    model.train()\n",
    "    return sampled_ix_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dbb47",
   "metadata": {},
   "source": [
    "Run finally the next cell to generate some new mineral names. Of course, an LSTM model can be used on much longer temporal sequences. Here we simply showed how to use an LSTM instead of a vanilla RNN to predict mineral names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ae3b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 => nekucite<EOS>\n",
      "10 => julinite<EOS>\n",
      "10 => jishite<EOS>\n",
      "13 => mellite<EOS>\n",
      "15 => osmite<EOS>\n",
      "15 => olivite<EOS>\n",
      "23 => weimony<EOS>\n",
      "10 => jurmosellite<EOS>\n",
      "12 => lazleucite<EOS>\n",
      "23 => weitgite<EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    seed = random.choice(list(char_to_ix.values())[1:])\n",
    "    print(seed, \"=>\", \"\".join(keys_to_values(\n",
    "        sample(model, seed, 5, False, 30, char_to_ix[\"<EOS>\"]),\n",
    "        ix_to_char, \"<?>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a2a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
